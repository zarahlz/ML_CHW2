{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC3RF8meAlK"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**: Zahra Helalizadeh\n",
        "\n",
        "**Student ID**: 400102193\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IraiR0SbeDi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQjwWC3eDnc"
      },
      "source": [
        "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "aNjSvryD0UEJ"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, precision_score, f1_score, recall_score, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "class MyLogisticRegression:\n",
        "    # Your code goes here!\n",
        "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, verbose=False):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.verbose = verbose\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            A = self.sigmoid(z)\n",
        "\n",
        "            # Compute gradients\n",
        "            dz = A - y\n",
        "            dw = (1 / m) * np.dot(X.T, dz)\n",
        "            db = (1 / m) * np.sum(dz)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            if self.verbose and i % 100 == 0:\n",
        "                cost = (-1 / m) * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
        "                print(f\"Iteration {i}: Cost = {cost}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        A = self.sigmoid(z)\n",
        "        y_pred = np.round(A)\n",
        "        return pd.DataFrame(y_pred, columns=['Target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i-oubUlZ6e"
      },
      "source": [
        "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXzIy_2u-pG",
        "outputId": "86d49b54-f375-41f2-adba-139232458b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.875\n",
            "Precision: 0.875\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9333333333333333\n",
            "Mean Squared Error: 0.125\n",
            "Root Mean Squared Error: 0.3535533905932738\n",
            "Confusion Matrix:\n",
            " [[ 0 10]\n",
            " [ 0 70]]\n",
            "R^2 Score: -0.1428571428571428\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, accuracy_score, confusion_matrix, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('Q2/Logistic_question.csv')\n",
        "# Convert Target column to binary\n",
        "df['Target'] = df['Target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Target'])\n",
        "y = df['Target']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize data using X_train\n",
        "def normalize(dataset):\n",
        "    normalized_df = (dataset - dataset.mean()) / dataset.std()\n",
        "    return normalized_df\n",
        "\n",
        "X_train_norm = normalize(X_train)\n",
        "X_test_norm = normalize(X_test)\n",
        "\n",
        "# Initialize and train MyLogisticRegression model\n",
        "lr_model = MyLogisticRegression()\n",
        "lr_model.fit(X_train_norm, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_test = lr_model.predict(X_test_norm)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"R^2 Score:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0RXNGKv1pa"
      },
      "source": [
        "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldveD35twRRZ"
      },
      "source": [
        "**Your answer:** Here's a brief explanation of each evaluation metric and the situations where they convey more data on the model performance:\n",
        "\n",
        "1. **Accuracy:** Accuracy measures the proportion of correct predictions among all predictions made by the model. It is a general metric that is easy to interpret and understand. However, accuracy may not be suitable for imbalanced datasets, where one class dominates the other. In such cases, accuracy can be misleading, and other metrics like precision, recall, or F1 score may provide a more comprehensive view of the model's performance.\n",
        "\n",
        "2. **Precision:** Precision measures the proportion of true positive predictions among all positive predictions made by the model. It indicates how many of the positively predicted instances are actually positive. Precision is useful in tasks where false positives are costly or undesirable. For example, in a medical diagnosis task, precision is crucial because it measures the proportion of correctly identified cases among all identified cases.\n",
        "\n",
        "3. **Recall (Sensitivity):** Recall measures the proportion of true positive predictions among all actual positive instances in the dataset. It indicates the model's ability to capture all positive instances. Recall is important in tasks where false negatives are costly or detrimental. For instance, in a cancer detection task, recall is essential because it measures the proportion of correctly identified cancer cases among all actual cancer cases.\n",
        "\n",
        "4. **F1 Score:** The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is particularly useful when there is an uneven class distribution or when false positives and false negatives have different implications. The F1 score is a good overall measure of a model's performance, especially when you want to strike a balance between precision and recall.\n",
        "\n",
        "5. **Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):** These metrics are commonly used in regression tasks to measure the average squared difference between the predicted values and the actual values. They provide insights into the model's accuracy in estimating continuous outcomes. MSE and RMSE are sensitive to outliers and can penalize large prediction errors heavily. They are useful when you want to quantify the magnitude of prediction errors in the model.\n",
        "\n",
        "6. **Confusion Matrix:** A confusion matrix is a table that summarizes the model's predictions against the actual labels in a tabular format. It provides a detailed breakdown of true positive, true negative, false positive, and false negative predictions. A confusion matrix is helpful for understanding the types of errors made by the model and can guide further improvements in model performance.\n",
        "\n",
        "7. **R^2 Score:** The R^2 score, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1 and represents the goodness of fit of the model. A higher R^2 score indicates a better fit of the model to the data. R^2 score is commonly used in regression tasks to assess the predictive power of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCeRHZSw-mh"
      },
      "source": [
        "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb5lRSQXDLR3",
        "outputId": "aacc2392-3864-4496-dc13-e50d2c84e888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.925\n",
            "Precision: 0.9210526315789473\n",
            "Recall: 1.0\n",
            "F1 Score: 0.958904109589041\n",
            "Confusion Matrix:\n",
            " [[ 4  6]\n",
            " [ 0 70]]\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train Logistic Regression model using scikit-learn\n",
        "lr_model_new = LogisticRegression()\n",
        "lr_model_new.fit(X_train_norm, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_test = lr_model_new.predict(X_test_norm)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvIymmMy_ji"
      },
      "source": [
        "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0ohM16z3De"
      },
      "source": [
        "**Your answer:** Here's a comparison between your custom logistic regression function and the built-in logistic regression function from scikit-learn, focusing on performance and parameters:\n",
        "\n",
        "**Performance:**\n",
        "- **Custom Logistic Regression Function:**\n",
        "  - The performance of the custom logistic regression function depends on the implementation details and optimization techniques used in the code.\n",
        "  - Since it's a custom implementation, performance may vary based on factors such as efficiency of gradient descent optimization, convergence criteria, and numerical stability.\n",
        "  - The custom function may not be as optimized or robust as the built-in logistic regression function.\n",
        "\n",
        "- **Built-in Logistic Regression Function (scikit-learn):**\n",
        "  - The built-in logistic regression function from scikit-learn is well-optimized and extensively tested, offering high-performance implementations of logistic regression algorithms.\n",
        "  - Scikit-learn's logistic regression function uses efficient optimization techniques, such as stochastic gradient descent or coordinate descent, to converge quickly and accurately.\n",
        "  - It benefits from various optimizations and enhancements, including regularization, parallelization, and support for sparse matrices, making it suitable for large-scale datasets.\n",
        "\n",
        "**Parameters:**\n",
        "- **Custom Logistic Regression Function:**\n",
        "  - The custom function has parameters such as learning rate (`learning_rate`), number of iterations (`num_iterations`), and verbosity (`verbose`).\n",
        "  - These parameters control the optimization process and the behavior of the algorithm during training.\n",
        "  - For example, the learning rate determines the step size in gradient descent, while the number of iterations specifies the maximum number of iterations for optimization.\n",
        "\n",
        "- **Built-in Logistic Regression Function (scikit-learn):**\n",
        "  - Scikit-learn's logistic regression function offers a wide range of parameters that control various aspects of the logistic regression model.\n",
        "  - Some important parameters include regularization strength (`C` or `alpha`), penalty type (`l1` or `l2`), solver algorithm (`liblinear`, `lbfgs`, `sag`, `saga`), and class weights (`class_weight`).\n",
        "  - These parameters allow fine-tuning of the logistic regression model to achieve better performance and address specific challenges, such as overfitting, class imbalance, and computational efficiency.\n",
        "\n",
        "**Effect of Parameters on Model's Performance:**\n",
        "- Parameters like regularization strength (`C` or `alpha`) and penalty type (`l1` or `l2`) have a significant impact on the model's ability to generalize and handle overfitting.\n",
        "- The choice of solver algorithm (`solver`) affects the optimization process and can influence convergence speed and memory usage.\n",
        "- Class weights (`class_weight`) can be used to address class imbalance by assigning different weights to different classes during training.\n",
        "\n",
        "Overall, while custom implementations offer flexibility and transparency, built-in functions like scikit-learn's logistic regression provide robust, optimized, and feature-rich solutions that are suitable for a wide range of tasks and datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMqoYlr2kr7"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvlqDe52xP5"
      },
      "source": [
        "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5Ir-_hFt286t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyMultinomialLogisticRegression:\n",
        "    def __init__(self, num_features, num_classes, learning_rate=0.01):\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.zeros((num_features, num_classes))\n",
        "        self.biases = np.zeros(num_classes)\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "    def loss_function(self, X, y):\n",
        "        logits = np.dot(X, self.weights) + self.biases\n",
        "        probabilities = self.softmax(logits)\n",
        "        loss = -np.mean(np.log(probabilities[np.arange(len(y)), y]))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=100, batch_size=32):\n",
        "        num_samples = X_train.shape[0]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(num_samples)\n",
        "            X_train_shuffled = X_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                X_batch = X_train_shuffled[i:i+batch_size]\n",
        "                y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "                logits = np.dot(X_batch, self.weights) + self.biases\n",
        "                probabilities = self.softmax(logits)\n",
        "\n",
        "                # Compute gradients\n",
        "                error = probabilities.copy()\n",
        "                error[np.arange(len(y_batch)), y_batch] -= 1\n",
        "                gradients_w = np.dot(X_batch.T, error) / len(y_batch)\n",
        "                gradients_b = np.mean(error, axis=0)\n",
        "\n",
        "                # Update weights and biases\n",
        "                self.weights -= self.learning_rate * gradients_w\n",
        "                self.biases -= self.learning_rate * gradients_b\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        logits = np.dot(X_test, self.weights) + self.biases\n",
        "        probabilities = self.softmax(logits)\n",
        "        return np.argmax(probabilities, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ3Rtay3Y2_"
      },
      "source": [
        "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aP4QJPq29B3",
        "outputId": "f19a0b27-208f-4229-a158-e604cec0f7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with 2 quantization levels: 0.8750\n",
            "Accuracy with 3 quantization levels: 0.8750\n",
            "Accuracy with 4 quantization levels: 0.8750\n",
            "Accuracy with 5 quantization levels: 0.8750\n",
            "Accuracy with 6 quantization levels: 0.9000\n",
            "Accuracy with 7 quantization levels: 0.8750\n",
            "Accuracy with 8 quantization levels: 0.8750\n",
            "Accuracy with 9 quantization levels: 0.8750\n",
            "Accuracy with 10 quantization levels: 0.8750\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import warnings\n",
        "\n",
        "# Convert dataframe to numpy arrays\n",
        "X_numpy = X.to_numpy()\n",
        "y_numpy = y.to_numpy()\n",
        "\n",
        "# Define the range of levels for quantization\n",
        "quantization_levels = range(2, 11)\n",
        "\n",
        "# Iterate over each level of quantization\n",
        "for i in quantization_levels:\n",
        "    # Quantize the target into i levels\n",
        "    est = KBinsDiscretizer(n_bins=i, encode='ordinal', strategy='uniform')\n",
        "    y_quantized = est.fit_transform(y_numpy.reshape(-1, 1)).flatten().astype(int)\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_quantized, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Instantiate and train the model\n",
        "    model = MyMultinomialLogisticRegression(num_features=X.shape[1], num_classes=i)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict labels for test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy with {i} quantization levels: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2sHl5Z4dXi"
      },
      "source": [
        "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLERDAr4wnS"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT43jGKV6CBZ"
      },
      "source": [
        "# Going a little further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9uGo0R6GZo"
      },
      "source": [
        "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "o-vrjYBF7u1E",
        "outputId": "6472177f-115d-4196-d004-763443a15d42"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m files\u001b[38;5;241m.\u001b[39mupload()  \u001b[38;5;66;03m# Use this to select the kaggle.json file from your computer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir -p ~/.kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Use this to select the kaggle.json file from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6u6_1v8ftX"
      },
      "source": [
        "Then use this code to automatically download the dataset into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjyVaVKF29Hx",
        "outputId": "777ed615-d205-49fd-f1e3-cf128a906bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset\n",
            "License(s): unknown\n",
            "adult-income-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/adult-income-dataset.zip\n",
            "replace adult.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
        "!unzip /content/adult-income-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQnbZwt8rJK"
      },
      "source": [
        "**Task:** Determine the number of null entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtuEx6QW29c1",
        "outputId": "2849ef6b-7ebf-4a0d-fccf-5e2c3ae0823e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of null entries: 6465\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "df = pd.read_csv('adult.csv')\n",
        "df.head()\n",
        "null_cnt = (df == '?').sum()\n",
        "all_null_cnt = null_cnt.sum()\n",
        "print('number of null entries:',all_null_cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEcBdTUAYVN"
      },
      "source": [
        "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1u1pBHuAsSg"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhH-hkpAxFf"
      },
      "source": [
        "**Task:** Handle null entries using your best method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fVwWcjK29fk",
        "outputId": "9f226c8c-8deb-493b-af44-98861e362d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of null entries: 0\n"
          ]
        }
      ],
      "source": [
        "# # Your code goes here!\n",
        "# numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
        "\n",
        "# # Impute null values in categorical columns with the most frequent value\n",
        "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "# most_frequent_value = df[categorical_cols].mode().iloc[0]  # Calculate the most frequent value\n",
        "# df[categorical_cols] = df[categorical_cols].fillna(most_frequent_value, inplace=True)\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "\n",
        "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n",
        "\n",
        "# Impute null values with mode for categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
        "\n",
        "null_cnt = df.isnull().sum()\n",
        "all_null_cnt = null_cnt.sum()\n",
        "print('number of null entries:',all_null_cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k5cTorCJaV"
      },
      "source": [
        "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agj18Lcd-vyZ",
        "outputId": "8d402a36-d89a-4c9e-d1f6-3756b83d9f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'classifier__C': 10, 'classifier__penalty': 'l2'}\n",
            "Train Accuracy: 0.8509456658050316\n",
            "Test Accuracy: 0.8559729757395844\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (replace 'data.csv' with your dataset file)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['income'])\n",
        "y = df['income']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps for numerical and categorical features\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and Logistic Regression\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(max_iter=1000))])\n",
        "# Define hyperparameters grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10, 100],\n",
        "    'classifier__penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', error_score='raise')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = accuracy_score(y_train, best_model.predict(X_train))\n",
        "test_accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lzr2lqXDQ1T"
      },
      "source": [
        "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9D1jlstF9nF",
        "outputId": "39b9bc94-1349-49ca-b94c-54fcea764a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score for Mean of probabilities: 0.8556658818712253\n",
            "Accuracy score for Majority Voting: 0.8555635172484389\n",
            "Accuracy score for Weighted Average: 0.8556658818712253\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of models (i)\n",
        "num_models = 10\n",
        "\n",
        "# Initialize a list to store the probabilities from each model\n",
        "model_results = []\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kf = KFold(n_splits=num_models)\n",
        "\n",
        "# Define preprocessing steps\n",
        "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Train i separate models and store the results\n",
        "for train_index, _ in kf.split(X_train):\n",
        "    # Split the training data\n",
        "    X_train_fold = X_train.iloc[train_index]\n",
        "    y_train_fold = y_train.iloc[train_index]\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train_fold)\n",
        "\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_preprocessed, y_train_fold)\n",
        "\n",
        "    # Store the probabilities from the model\n",
        "    model_results.append(model.predict_proba(preprocessor.transform(X_test))[:, 1])\n",
        "\n",
        "# Ensemble Method 1: Mean of probabilities\n",
        "ensemble_predictions_mean = np.mean(model_results, axis=0)\n",
        "\n",
        "# Ensemble Method 2: Majority Voting\n",
        "ensemble_predictions_majority = np.mean(np.array(model_results) > 0.5, axis=0)\n",
        "\n",
        "# Ensemble Method 3: Weighted Average (based on model performance)\n",
        "# For simplicity, let's assume equal weights for now\n",
        "ensemble_predictions_weighted = np.mean(model_results, axis=0)\n",
        "predicted_labels_mean = np.where(ensemble_predictions_mean >= 0.5, '>50K', '<=50K')\n",
        "predicted_labels_majority = np.where(ensemble_predictions_majority >= 0.5, '>50K', '<=50K')\n",
        "predicted_labels_weighted = np.where(ensemble_predictions_weighted >= 0.5, '>50K', '<=50K')\n",
        "\n",
        "accuracy_mean = accuracy_score(y_test, predicted_labels_mean)\n",
        "accuracy_majority = accuracy_score(y_test, predicted_labels_majority)\n",
        "accuracy_weighted = accuracy_score(y_test, predicted_labels_weighted)\n",
        "\n",
        "# Print accuracy scores for each ensemble method\n",
        "print(\"Accuracy score for Mean of probabilities:\", accuracy_mean)\n",
        "print(\"Accuracy score for Majority Voting:\", accuracy_majority)\n",
        "print(\"Accuracy score for Weighted Average:\", accuracy_weighted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS9HYJ5FW1T"
      },
      "source": [
        "**Question:** Explain your proposed methods and the reason you decided to use them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCBQuAeF46a"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSREvg4FTHf"
      },
      "source": [
        "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tfKS-Jq0-v4P",
        "outputId": "8cd70405-0ce6-46ed-fa80-7ae4d37ed6da"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Singleton array array(0) cannot be considered a valid collection.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-46ef57f43c87>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Calculate accuracy scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0maccuracy_mean_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_mean_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0maccuracy_mean_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_mean_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0maccuracy_majority_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_majority_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Singleton array array(0) cannot be considered a valid collection."
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define range for i\n",
        "num_models_range = range(2, 101)\n",
        "\n",
        "# Initialize lists to store accuracy scores\n",
        "accuracy_mean_train = []\n",
        "accuracy_mean_test = []\n",
        "accuracy_majority_train = []\n",
        "accuracy_majority_test = []\n",
        "accuracy_weighted_train = []\n",
        "accuracy_weighted_test = []\n",
        "\n",
        "best_accuracy = 0\n",
        "best_i = 0\n",
        "\n",
        "for num_models in num_models_range:\n",
        "    # Initialize lists to store the probabilities from each model\n",
        "    model_results_train = []\n",
        "    model_results_test = []\n",
        "\n",
        "    # Initialize KFold cross-validation\n",
        "    kf = KFold(n_splits=num_models)\n",
        "\n",
        "    # Train i separate models and store the results\n",
        "    for train_index, test_index in kf.split(X_train):\n",
        "        # Get the indices for X_train and y_train\n",
        "        X_train_fold = X_train.iloc[train_index]\n",
        "        y_train_fold = y_train.iloc[train_index]\n",
        "        X_test_fold = X_train.iloc[test_index]\n",
        "\n",
        "        # Preprocess data\n",
        "        numeric_features = X_train_fold.select_dtypes(include=['float64', 'int64']).columns\n",
        "        categorical_features = X_train_fold.select_dtypes(include=['object']).columns\n",
        "\n",
        "        numeric_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        X_train_fold_preprocessed = preprocessor.fit_transform(X_train_fold)\n",
        "        X_test_fold_preprocessed = preprocessor.transform(X_test_fold)\n",
        "\n",
        "        # Train Logistic Regression model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train_fold_preprocessed, y_train_fold)\n",
        "\n",
        "        # Store the probabilities from the model\n",
        "        model_results_train.append(model.predict_proba(X_train_fold_preprocessed)[:, 1])\n",
        "        model_results_test.append(model.predict_proba(X_test_fold_preprocessed)[:, 1])\n",
        "\n",
        "    # Concatenate arrays within model_results_train and model_results_test\n",
        "    ensemble_predictions_mean_train = np.mean(np.concatenate(model_results_train), axis=0)\n",
        "    ensemble_predictions_mean_test = np.mean(np.concatenate(model_results_test), axis=0)\n",
        "    predicted_labels_mean_train = np.where(ensemble_predictions_mean_train >= 0.5, 1, 0)\n",
        "    predicted_labels_mean_test = np.where(ensemble_predictions_mean_test >= 0.5, 1, 0)\n",
        "\n",
        "    # Ensemble Method 2: Majority Voting\n",
        "    ensemble_predictions_majority_train = np.mean(np.concatenate(model_results_train) > 0.5, axis=0)\n",
        "    ensemble_predictions_majority_test = np.mean(np.concatenate(model_results_test) > 0.5, axis=0)\n",
        "    predicted_labels_majority_train = np.where(ensemble_predictions_majority_train >= 0.5, 1, 0)\n",
        "    predicted_labels_majority_test = np.where(ensemble_predictions_majority_test >= 0.5, 1, 0)\n",
        "\n",
        "    # Ensemble Method 3: Weighted Average (based on model performance)\n",
        "    weights = np.ones(len(np.concatenate(model_results_train))) / len(np.concatenate(model_results_train))\n",
        "    ensemble_predictions_weighted_train = np.average(np.concatenate(model_results_train), axis=0, weights=weights)\n",
        "    ensemble_predictions_weighted_test = np.average(np.concatenate(model_results_test), axis=0, weights=weights)\n",
        "    predicted_labels_weighted_train = np.where(ensemble_predictions_weighted_train >= 0.5, 1, 0)\n",
        "    predicted_labels_weighted_test = np.where(ensemble_predictions_weighted_test >= 0.5, 1, 0)\n",
        "\n",
        "    # Calculate accuracy scores\n",
        "    accuracy_mean_train.append(accuracy_score(y_train, predicted_labels_mean_train))\n",
        "    accuracy_mean_test.append(accuracy_score(y_test, predicted_labels_mean_test))\n",
        "    accuracy_majority_train.append(accuracy_score(y_train, predicted_labels_majority_train))\n",
        "    accuracy_majority_test.append(accuracy_score(y_test, predicted_labels_majority_test))\n",
        "    accuracy_weighted_train.append(accuracy_score(y_train, predicted_labels_weighted_train))\n",
        "    accuracy_weighted_test.append(accuracy_score(y_test, predicted_labels_weighted_test))\n",
        "\n",
        "    # Update best accuracy and i\n",
        "    if max(accuracy_mean_test[-1], accuracy_majority_test[-1], accuracy_weighted_test[-1]) > best_accuracy:\n",
        "        best_accuracy = max(accuracy_mean_test[-1], accuracy_majority_test[-1], accuracy_weighted_test[-1])\n",
        "        best_i = num_models\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_models_range, accuracy_mean_train, label='Mean of probabilities (Train)', color='blue')\n",
        "plt.plot(num_models_range, accuracy_mean_test, label='Mean of probabilities (Test)', linestyle='--', color='blue')\n",
        "plt.plot(num_models_range, accuracy_majority_train, label='Majority Voting (Train)', color='red')\n",
        "plt.plot(num_models_range, accuracy_majority_test, label='Majority Voting (Test)', linestyle='--', color='red')\n",
        "plt.plot(num_models_range, accuracy_weighted_train, label='Weighted Average (Train)', color='green')\n",
        "plt.plot(num_models_range, accuracy_weighted_test, label='Weighted Average (Test)', linestyle='--', color='green')\n",
        "plt.xlabel('Number of Models (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Models')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best value of i: {best_i}\")\n",
        "print(f\"Best test accuracy: {best_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV0YUgRGg1p"
      },
      "source": [
        "**Question:** Analyze the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yln4x0Y10UES"
      },
      "source": [
        "**Your Answer:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
